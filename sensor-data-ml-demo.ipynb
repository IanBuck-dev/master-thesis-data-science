{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isnan\n",
    "from collections import Counter\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import box\n",
    "from sklearn.model_selection import train_test_split\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.transform import Affine\n",
    "\n",
    "\n",
    "# set print options\n",
    "np.set_printoptions(linewidth=110)\n",
    "pd.set_option('display.width', 200)\n",
    "plt.style.use('bmh')  # better for plotting geometries vs general plots.\n",
    "\n",
    "# Import only first 1000 lines for testing\n",
    "df = pd.read_csv('./data/sensor_readings_timeseries_part2.csv', nrows=500, parse_dates=[\"Timestamp\"])\n",
    "df = df.drop(\n",
    "    [\"Metadata.Location.type\", \"Metadata.SensorCommunitySensorType\", \"Metadata.SensorType\", \"_id\"], axis=1)\n",
    "\n",
    "# Convert string coordinates to list\n",
    "df[\"Metadata.Location.coordinates\"] = df[\"Metadata.Location.coordinates\"].apply(\n",
    "    lambda x: ast.literal_eval(x))\n",
    "\n",
    "# no preprocessing needed\n",
    "sensor_community_df = df.loc[df[\"Metadata.Provider\"] == \"sensor.community\"].drop(\n",
    "    [\"Metadata.Provider\", \"Metadata.NetatmoSensorId\"], axis=1)\n",
    "\n",
    "# need to combine multiple entries for the same sensor id and time\n",
    "netatmo_df = df.loc[df[\"Metadata.Provider\"] == \"netatmo\"].drop(\n",
    "    [\"Metadata.Provider\"], axis=1)\n",
    "\n",
    "# get unique groups\n",
    "grouped = netatmo_df.groupby(\n",
    "    [\"Metadata.NetatmoSensorId\", \"Timestamp\"]).aggregate(list)\n",
    "\n",
    "combined_rows = []\n",
    "for label, group in grouped.iterrows():\n",
    "    humidity = next((x for x in group.values[0] if not isnan(x)), np.NaN)\n",
    "    pressure = next((x for x in group.values[2] if not isnan(x)), np.NaN)\n",
    "    temperature = next((x for x in group.values[3] if not isnan(x)), np.NaN)\n",
    "\n",
    "    # Humidity, Metadata.Location.coordinates, Pressure, Temperature, Timestamp\n",
    "    entry = [humidity, group.values[1][0], pressure, temperature, label[1]]\n",
    "    combined_rows.append(entry)\n",
    "\n",
    "netatmo_df = pd.DataFrame(combined_rows, columns=[\n",
    "                          \"Humidity\", \"Metadata.Location.coordinates\", \"Pressure\", \"Temperature\", \"Timestamp\"])\n",
    "\n",
    "# Combine prepared data\n",
    "prep = pd.concat([sensor_community_df, netatmo_df], ignore_index=True, axis=0)\n",
    "prep = prep[prep[\"Temperature\"].notna()]\n",
    "\n",
    "coordinates = prep[\"Metadata.Location.coordinates\"]\n",
    "proj_wgs = 4326\n",
    "\n",
    "geo_df = GeoDataFrame(prep[[\"Humidity\", \"Pressure\", \"Temperature\", \"Timestamp\"]],\n",
    "                          geometry=prep[\"Metadata.Location.coordinates\"].apply(lambda row: Point(row[0], row[1])), crs=proj_wgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "from geopandas.tools import sjoin\n",
    "\n",
    "# Hamburg boundaries\n",
    "# hamburg = gpd.read_file('./data/hamburg.shx')\n",
    "# hamburg = hamburg.to_crs(proj_wgs)\n",
    "\n",
    "# hamburg = hamburg.clip_by_rect(geo_df.total_bounds[0], geo_df.total_bounds[1], geo_df.total_bounds[2], geo_df.total_bounds[3])\n",
    "\n",
    "# basisview = gpd.read_file(\n",
    "#     './data/basisviews_bdlm_HH_EPSG_4326_2023-04-25.gpkg')\n",
    "# basisview = basisview.to_crs(proj_wgs)\n",
    "\n",
    "# basisview = basisview.clip_by_rect(geo_df.total_bounds[0], geo_df.total_bounds[1], geo_df.total_bounds[2], geo_df.total_bounds[3])\n",
    "\n",
    "ge5000 = gpd.read_file(\n",
    "    './data/ge5000.utm32s.shape/ge5000/vrm/VRM5000.shp')\n",
    "ge5000 = ge5000.to_crs(proj_wgs)\n",
    "\n",
    "# ge5000 = ge5000.clip_by_rect(geo_df.total_bounds[0], geo_df.total_bounds[1], geo_df.total_bounds[2], geo_df.total_bounds[3])\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "plt.style.use('bmh')\n",
    "\n",
    "ge5000.plot(ax=ax, color='white', edgecolor='black')\n",
    "# hamburg.plot(ax=ax, color='white', edgecolor='black')\n",
    "geo_df.plot(ax=ax, marker='o', color='royalblue', markersize=3)\n",
    "\n",
    "\n",
    "ax.set_title('Temperature in Hamburg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "from pytz import UTC\n",
    "\n",
    "start_time = pd.to_datetime('2023-02-02T20:00:00.000000000').tz_localize(UTC)\n",
    "end_time = pd.to_datetime('2023-02-02T20:05:00.000000000').tz_localize(UTC)\n",
    "filtered_gdf = geo_df.loc[(geo_df['Timestamp'] >= start_time)\n",
    "                          & (geo_df['Timestamp'] <= end_time)]\n",
    "\n",
    "filtered_gdf.plot()\n",
    "\n",
    "def to_numpy_list(geo_series):\n",
    "    return [[point.x, point.y]\n",
    "            for point in geo_series.to_numpy()]\n",
    "\n",
    "def geo_df_to_train_test_sets(gdf: gpd.GeoDataFrame, target_feature: str = 'Temperature') :\n",
    "    \"\"\"\n",
    "    Splits a GeoDataFrame into train and test sets.\n",
    "    :param gdf: The GeoDataFrame to split.\n",
    "    :param target_feature: The name of the target feature. Can be any column in the GeoDataFrame: 'Temperature', 'Humidity', 'Pressure'.\n",
    "    \"\"\"\n",
    "    coords_train, coords_test, value_train, value_test = train_test_split(\n",
    "        gdf.geometry, gdf[target_feature], test_size=0.2, random_state=0)\n",
    "    \n",
    "    coords_train = to_numpy_list(coords_train)\n",
    "    coords_test = to_numpy_list(coords_test)\n",
    "\n",
    "    return coords_train, coords_test, value_train, value_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "knn_regressor = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')\n",
    "\n",
    "coords_geo_train, coords_geo_test, value_geo_train, value_geo_test = geo_df_to_train_test_sets(\n",
    "    filtered_gdf)\n",
    "\n",
    "# Fit regressor to data\n",
    "knn_regressor.fit(coords_geo_train, value_geo_train)\n",
    "\n",
    "# Generate out-of-sample R^2\n",
    "out_r_squared_knn = knn_regressor.score(coords_geo_test, value_geo_test)\n",
    "print(\"KNN out-of-sample r-squared: {}\".format(round(out_r_squared_knn, 2)))\n",
    "\n",
    "# Predict values for testing dataset\n",
    "coords_rain_test_predict_knn = knn_regressor.predict(coords_geo_test)\n",
    "\n",
    "# Create dictionary holding the actual and predicted values\n",
    "predict_dict_knn = {\"Coordinate_Pair\": coords_geo_test,\n",
    "                    \"VALUE_Actual\": value_geo_test, \"VALUE_Predict\": coords_rain_test_predict_knn}\n",
    "\n",
    "# Create dataframe from dictionary\n",
    "predict_df_knn = pd.DataFrame(predict_dict_knn)\n",
    "\n",
    "# Display attribute table\n",
    "print(\"\\nAttribute Table: Testing Set Interpolated Values - KNN Method\")\n",
    "display(predict_df_knn.head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging Reference Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename):\n",
    "    '''Export and save a kernel density raster.'''\n",
    "\n",
    "    # Get resolution\n",
    "    xres = (max_x - min_x) / len(XX)\n",
    "    yres = (max_y - min_y) / len(YY)\n",
    "\n",
    "    # Set transform\n",
    "    transform = Affine.translation(\n",
    "        min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres)\n",
    "\n",
    "    # Export array as raster\n",
    "    with rasterio.open(\n",
    "            filename,\n",
    "            mode=\"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=Z.shape[0],\n",
    "            width=Z.shape[1],\n",
    "            count=1,\n",
    "            dtype=Z.dtype,\n",
    "            crs=proj,\n",
    "            transform=transform,\n",
    "    ) as new_dataset:\n",
    "        new_dataset.write(Z, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Krigin interpolation\n",
    "# see: https://pygis.io/docs/e_interpolation.html#kriging\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Define functions\n",
    "def interpolate_ordinary_kriging(base_gdf: GeoDataFrame, variogram_model: str, target_feature: str = 'Temperature', normalize: bool = False):\n",
    "    \"\"\"\n",
    "    Interpolate data using ordinary kriging.\n",
    "\n",
    "    :param base_gdf: GeoDataFrame containing the data to be interpolated\n",
    "    :param variogram_model: The variogram model to be used for interpolation. Possible values: linear, power, gaussian, spherical, exponential, hole-effect\n",
    "    :param target_feature: The name of the target feature. Can be any column in the GeoDataFrame: 'Temperature', 'Humidity', 'Pressure'.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values\n",
    "    base_gdf = GeoDataFrame(base_gdf.dropna(subset=[target_feature]))\n",
    "\n",
    "    # Normalize data\n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "        base_gdf[target_feature] = min_max_scaler.fit_transform(\n",
    "            base_gdf[target_feature].values.reshape(-1, 1))\n",
    "\n",
    "    x_coords = base_gdf.geometry.x\n",
    "    y_coords = base_gdf.geometry.y\n",
    "\n",
    "    coords_train, coords_test, value_train, value_test = geo_df_to_train_test_sets(\n",
    "        base_gdf, target_feature)\n",
    "    \n",
    "    # Get outer bounds for data points\n",
    "    minx, miny, maxx, maxy = geo_df.total_bounds\n",
    "\n",
    "    # Add buffer to outer bounds\n",
    "    minx -= .2\n",
    "    miny -= .1\n",
    "    maxx += .2\n",
    "    maxy += .1\n",
    "\n",
    "    # Create a 100 by 100 grid from the outer bounds\n",
    "    grid_lon = np.linspace(minx, maxx, 100)\n",
    "    grid_lat = np.linspace(miny, maxy, 100)\n",
    "\n",
    "    # Generate ordinary krigin object\n",
    "    OK = OrdinaryKriging(\n",
    "        [item[0] for item in coords_train],\n",
    "        [item[1] for item in coords_train],\n",
    "        value_train,\n",
    "        variogram_model=variogram_model,\n",
    "        verbose=False,\n",
    "        enable_plotting=False,\n",
    "        coordinates_type=\"euclidean\",\n",
    "    )\n",
    "\n",
    "    # Evaluate the method on grid\n",
    "    Z_pk_krig, sigma_squared_p_krig = OK.execute(\"grid\", grid_lon, grid_lat)\n",
    "\n",
    "    filename = f'./data/hamburg_{target_feature}_krigin_{variogram_model}.tif'\n",
    "\n",
    "    # Export raster\n",
    "    export_kde_raster(Z=Z_pk_krig, XX=grid_lon, YY=grid_lat, min_x=minx, max_x=maxx, min_y=miny,\n",
    "                  max_y=maxy, proj=proj_wgs, filename=filename)\n",
    "\n",
    "    # Open raster\n",
    "    raster_pk = rasterio.open(filename)\n",
    "\n",
    "    # Create polygon with extend of raster\n",
    "    poly_shapely = box(*raster_pk.bounds)\n",
    "\n",
    "    # Create a dictionary with needed attributes and required geometry column\n",
    "    attributes_df = {'Attribute': ['name1'], 'geometry': poly_shapely}\n",
    "\n",
    "    # Convert shapely object to a GeoDataFrame\n",
    "    raster_pk_extent = GeoDataFrame(\n",
    "        attributes_df, geometry='geometry', crs=proj_wgs)\n",
    "    \n",
    "    # Create copy of test dataset\n",
    "    temp_test_gdf_pk_krig = base_gdf.copy()\n",
    "\n",
    "    # Subset the GeoDataFrame by checking which test points are within the raster extent polygon\n",
    "    # If a test point is beyond the extent of training points dataset, the kriging output may not cover that test point\n",
    "    temp_test_gdf_pk_krig = temp_test_gdf_pk_krig[temp_test_gdf_pk_krig.within(\n",
    "        raster_pk_extent.geometry.values[0])]\n",
    "\n",
    "    # Create list of XY coordinate pairs for the test points that fall within raster extent polygon\n",
    "    coords_rain_test_pk_krig = [list(xy) for xy in zip(\n",
    "        temp_test_gdf_pk_krig[\"geometry\"].x, temp_test_gdf_pk_krig[\"geometry\"].y)]\n",
    "\n",
    "    # Extract raster value at each test point and add the values to the GeoDataFrame\n",
    "    temp_test_gdf_pk_krig[\"VALUE_Predict\"] = [x[0]\n",
    "                                            for x in raster_pk.sample(coords_rain_test_pk_krig)]\n",
    "\n",
    "    # Generate out-of-sample R^2\n",
    "    out_r_squared_tp = r2_score(\n",
    "        temp_test_gdf_pk_krig.Temperature, temp_test_gdf_pk_krig.VALUE_Predict)\n",
    "    print(\n",
    "        f'PyKrige Kriging out-of-sample r-squared for variogram model {variogram_model} and target feature {target_feature}: {round(out_r_squared_tp, 2)}')\n",
    "    \n",
    "    return raster_pk_extent, raster_pk, x_coords, y_coords, coords_train, coords_test, value_train, value_test, Z_pk_krig, sigma_squared_p_krig, poly_shapely\n",
    "\n",
    "\n",
    "def plot_kriging_data(input_raster, x_coords, y_coords, variogram_model: str, target_feature: str = 'Temperature', label: str = 'Temperature (°C)'):\n",
    "    # Mask raster to hamburg shape\n",
    "    out_image_pk, out_transform_pk = rasterio.mask.mask(\n",
    "        input_raster, ge5000_clipped.geometry.values, crop=True)\n",
    "    \n",
    "    # Reset plot\n",
    "    plt.clf()\n",
    "    \n",
    "    # Plot data\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    plot = show(out_image_pk, ax=ax, transform=out_transform_pk, cmap=\"RdPu\")\n",
    "    ax.plot(x_coords, y_coords, 'k.', markersize=2, alpha=0.5)\n",
    "    ge5000_clipped.plot(ax=ax, color='none', edgecolor='dimgray')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(f'Hamburg - Interpolating {target_feature} using Kriging with {variogram_model} variogram model',\n",
    "                fontdict={'fontsize': '15', 'fontweight': '3'})\n",
    "\n",
    "    # Display plot\n",
    "    plt.colorbar(plot.get_images()[0], label=label)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# Set projection to WGS 84 and reproject data\n",
    "krigin_gdf = filtered_gdf.to_crs(epsg=proj_wgs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Interpolation using Ordinary Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate temperature data using ordinary kriging with various variogram models to compare r2 scores\n",
    "raster_pk_extent_l, raster_pk_l, x_coords_l, y_coords_l, coords_train_l, coords_test_l, value_train_l, value_test_l, Z_pk_krig_l, sigma_squared_p_krig_l, poly_shapely_l = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'linear')\n",
    "raster_pk_extent_p, raster_pk_p, x_coords_p, y_coords_p, coords_train_p, coords_test_p, value_train_p, value_test_p, Z_pk_krig_p, sigma_squared_p_krig_p, poly_shapely_p = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'power')\n",
    "raster_pk_extent_g, raster_pk_g, x_coords_g, y_coords_g, coords_train_g, coords_test_g, value_train_g, value_test_g, Z_pk_krig_g, sigma_squared_p_krig_g, poly_shapely_g = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'gaussian')\n",
    "raster_pk_extent, raster_pk_s, x_coords_s, y_coords_s, coords_train_s, coords_test_s, value_train_s, value_test_s, Z_pk_krig_s, sigma_squared_p_krig_s, poly_shapely_s = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'spherical')\n",
    "raster_pk_extent_e, raster_pk_e, x_coords_e, y_coords_e, coords_train_e, coords_test_e, value_train_e, value_test_e, Z_pk_krig_e, sigma_squared_p_krig_e, poly_shapely_e = interpolate_ordinary_kriging(krigin_gdf, 'exponential')\n",
    "raster_pk_extent_h, raster_pk_h, x_coords_h, y_coords_h, coords_train_h, coords_test_h, value_train_h, value_test_h, Z_pk_krig_h, sigma_squared_p_krig_h, poly_shapely_h = interpolate_ordinary_kriging(krigin_gdf, 'hole-effect')\n",
    "\n",
    "# Interpolate humidity data using ordinary kriging with various variogram models to compare r2 scores\n",
    "raster_pk_extent_hum_l, raster_pk_hum_l, x_coords_hum_l, y_coords_hum_l, coords_train_hum_l, coords_test_hum_l, value_train_hum_l, value_test_hum_l, Z_pk_krig_hum_l, sigma_squared_p_krig_hum_l, poly_shapely_hum_l = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'linear', 'Humidity', True)\n",
    "\n",
    "\n",
    "ge5000_clipped = ge5000.clip(poly_shapely_l)\n",
    "\n",
    "# Plot temperature kriging results\n",
    "plot_kriging_data(raster_pk_l, x_coords_l, y_coords_l, 'linear')\n",
    "plot_kriging_data(raster_pk_p, x_coords_p, y_coords_p, 'power')\n",
    "plot_kriging_data(raster_pk_g, x_coords_g, y_coords_g, 'gaussian')\n",
    "plot_kriging_data(raster_pk_s, x_coords_s, y_coords_s, 'spherical')\n",
    "plot_kriging_data(raster_pk_e, x_coords_e, y_coords_e, 'exponential')\n",
    "plot_kriging_data(raster_pk_h, x_coords_h, y_coords_h, 'hole-effect')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humidity Interpolation using Ordinary Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate humidity data using ordinary kriging with various variogram models to compare r2 scores\n",
    "raster_pk_extent_hum_l, raster_pk_hum_l, x_coords_hum_l, y_coords_hum_l, coords_train_hum_l, coords_test_hum_l, value_train_hum_l, value_test_hum_l, Z_pk_krig_hum_l, sigma_squared_p_krig_hum_l, poly_shapely_hum_l = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'linear', 'Humidity', True)\n",
    "\n",
    "# Same for power, gaussian, spherical, exponential and hole-effect\n",
    "raster_pk_extent_hum_p, raster_pk_hum_p, x_coords_hum_p, y_coords_hum_p, coords_train_hum_p, coords_test_hum_p, value_train_hum_p, value_test_hum_p, Z_pk_krig_hum_p, sigma_squared_p_krig_hum_p, poly_shapely_hum_p = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'power', 'Humidity', True)\n",
    "\n",
    "raster_pk_extent_hum_g, raster_pk_hum_g, x_coords_hum_g, y_coords_hum_g, coords_train_hum_g, coords_test_hum_g, value_train_hum_g, value_test_hum_g, Z_pk_krig_hum_g, sigma_squared_p_krig_hum_g, poly_shapely_hum_g = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'gaussian', 'Humidity', True)\n",
    "\n",
    "raster_pk_extent_hum_s, raster_pk_hum_s, x_coords_hum_s, y_coords_hum_s, coords_train_hum_s, coords_test_hum_s, value_train_hum_s, value_test_hum_s, Z_pk_krig_hum_s, sigma_squared_p_krig_hum_s, poly_shapely_hum_s = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'spherical', 'Humidity', True)\n",
    "\n",
    "raster_pk_extent_hum_e, raster_pk_hum_e, x_coords_hum_e, y_coords_hum_e, coords_train_hum_e, coords_test_hum_e, value_train_hum_e, value_test_hum_e, Z_pk_krig_hum_e, sigma_squared_p_krig_hum_e, poly_shapely_hum_e = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'exponential', 'Humidity', True)\n",
    "\n",
    "raster_pk_extent_hum_h, raster_pk_hum_h, x_coords_hum_h, y_coords_hum_h, coords_train_hum_h, coords_test_hum_h, value_train_hum_h, value_test_hum_h, Z_pk_krig_hum_h, sigma_squared_p_krig_hum_h, poly_shapely_hum_h = interpolate_ordinary_kriging(\n",
    "    krigin_gdf, 'hole-effect', 'Humidity', True)\n",
    "\n",
    "\n",
    "ge5000_clipped = ge5000.clip(poly_shapely_hum_l)\n",
    "\n",
    "# Plot humidity kriging results\n",
    "plot_kriging_data(raster_pk_hum_l, x_coords_hum_l,\n",
    "                  y_coords_hum_l, 'linear', 'Humidity', 'Humidity (%)')\n",
    "plot_kriging_data(raster_pk_hum_p, x_coords_hum_p,\n",
    "                    y_coords_hum_p, 'power', 'Humidity', 'Humidity (%)')\n",
    "plot_kriging_data(raster_pk_hum_g, x_coords_hum_g,\n",
    "                    y_coords_hum_g, 'gaussian', 'Humidity', 'Humidity (%)')\n",
    "plot_kriging_data(raster_pk_hum_s, x_coords_hum_s,\n",
    "                    y_coords_hum_s, 'spherical', 'Humidity', 'Humidity (%)')\n",
    "plot_kriging_data(raster_pk_hum_e, x_coords_hum_e,\n",
    "                    y_coords_hum_e, 'exponential', 'Humidity', 'Humidity (%)')\n",
    "plot_kriging_data(raster_pk_hum_h, x_coords_hum_h,\n",
    "                    y_coords_hum_h, 'hole-effect', 'Humidity', 'Humidity (%)')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML implementation part\n",
    "\n",
    "Foundation: TensorFlow with Keras (high level API)\n",
    "\n",
    "Idea: Have a neural network with all possible sensor locations, stationary and moving, and learn the dependencies between the different sensor locations\n",
    "- time-series based, so depending on the past improve the prediction quality\n",
    "- layers (maybe) need to be interconnected, so no simple feed-forward\n",
    "\n",
    "Model considerations:\n",
    "- bias:\n",
    "    - sample bias: (currently) only data from Hamburg area for a certain range of temperature etc. values for a small timeframe, only one climactic zone, close to water, where sensors are located -> placement, representability etc.\n",
    "    - measurement bias: low quality sensors are less accurate then reference grade equipment, local influences can skew correct readings\n",
    "    - exclusion bias: currently only a small number of features is measured, but maybe another factor (like soil temperature) has a high influence on the prediction quality and is missing\n",
    "- loss function: For regression tasks, there are 3 loss functions that are applicalble\n",
    "    - MSE (Mean Squared Error) -> default for regression problems, if distribution of the target variable is Gaussian, bigger punishment for larger prediction errors\n",
    "    - MSLE (mean squared logarithmic error) -> It has the effect of relaxing the punishing effect of large differences in large predicted values.\n",
    "\n",
    "Model candidates:\n",
    "- RNN (recurrent neural network)\n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Specific\n",
    "\n",
    "- train one layer with static properties (NDVI indexes etc.) and make it non-trainable later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference Random Regression Data Set for testing\n",
    "# generate regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras setup\n",
    "import tensorflow as tf\n",
    "from keras.losses import MSE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "# define input layer\n",
    "\n",
    "\n",
    "# define loss function\n",
    "model.compile(\n",
    "    loss=MSE,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01))\n",
    "\n",
    "# define output layer\n",
    "model.add(Dense(1, activation='linear'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4289530f3b2db6ec356c3e194078e4814479ad0c71c9eae9b7bdece64a60a424"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
